#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
RAG-based Footfall Data Analysis API Server (CPU Friendly Version)
ä½¿ç”¨ FAISS + TimesFM Embedding + OpenAI LLM çš„äººæµé‡æ•°æ®æŸ¥è¯¢æœåŠ¡
æ”¯æŒCPUå’ŒGPUè¿è¡Œï¼Œè‡ªåŠ¨æ£€æµ‹å’Œfallback
"""

import os
import json
import numpy as np
import faiss
import torch
from flask import Flask, request, jsonify
from dotenv import load_dotenv
from transformers import TimesFmModelForPrediction
from openai import OpenAI
import logging
from typing import List, Dict, Any, Tuple

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# åˆ›å»ºFlaskåº”ç”¨
app = Flask(__name__)

# å…¨å±€å˜é‡
model = None
client = None
faiss_index = None
records = []
embeddings = None
device_info = {}

def detect_device():
    """æ£€æµ‹å¹¶è¿”å›è®¾å¤‡ä¿¡æ¯"""
    global device_info

    # æ£€æµ‹CUDA
    cuda_available = torch.cuda.is_available()
    device_type = "cuda" if cuda_available else "cpu"

    device_info = {
        "type": device_type,
        "cuda_available": cuda_available,
        "gpu_count": torch.cuda.device_count() if cuda_available else 0,
        "cpu_count": os.cpu_count()
    }

    if cuda_available:
        device_info["gpu_name"] = torch.cuda.get_device_name(0)
        device_info["gpu_memory"] = torch.cuda.get_device_properties(0).total_memory / (1024**3)  # GB

    logger.info(f"ğŸ” Device detection: {device_info}")
    return device_info

def initialize_timesfm():
    """åˆå§‹åŒ– TimesFM æ¨¡å‹ - CPUå‹å¥½ç‰ˆæœ¬"""
    global model

    try:
        # æ£€æµ‹è®¾å¤‡
        device_info = detect_device()

        if device_info["cuda_available"]:
            logger.info("ğŸš€ Attempting to load TimesFM model on GPU...")
            try:
                model = TimesFmModelForPrediction.from_pretrained(
                    "google/timesfm-2.0-500m-pytorch",
                    torch_dtype=torch.bfloat16,
                    device_map="auto",
                    low_cpu_mem_usage=True
                )
                logger.info(f"âœ… TimesFM model loaded successfully on GPU: {device_info.get('gpu_name', 'Unknown')}")
                return True
            except Exception as gpu_error:
                logger.warning(f"âš ï¸ GPU loading failed: {gpu_error}")
                logger.info("ğŸ”„ Falling back to CPU...")

        # CPUæ¨¡å¼åŠ è½½
        logger.info("ğŸš€ Loading TimesFM model on CPU...")
        model = TimesFmModelForPrediction.from_pretrained(
            "google/timesfm-2.0-500m-pytorch",
            torch_dtype=torch.float32,  # CPUç”¨float32æ›´ç¨³å®š
            device_map="cpu",
            low_cpu_mem_usage=True
        )
        logger.info(f"âœ… TimesFM model loaded successfully on CPU ({device_info['cpu_count']} cores)")
        return True

    except Exception as e:
        logger.error(f"âŒ Failed to load TimesFM model: {str(e)}")
        logger.error("Troubleshooting tips:")
        logger.error("1. Ensure you have sufficient memory (8GB+ recommended)")
        logger.error("2. Check internet connection for model download")
        logger.error("3. Try reducing max_records in load_and_process_data()")
        return False

def timesfm_embed(series: np.ndarray) -> np.ndarray:
    """å¯¹æ•°å€¼åºåˆ—ç”Ÿæˆ embedding - è‡ªé€‚åº”è®¾å¤‡ç‰ˆæœ¬"""
    if model is None:
        raise ValueError("TimesFM model not initialized")

    device = model.device

    # æ ¹æ®è®¾å¤‡ç±»å‹é€‰æ‹©æ•°æ®ç±»å‹
    if device.type == "cuda":
        dtype = torch.bfloat16
    else:
        dtype = torch.float32

    try:
        x = torch.tensor(series, dtype=dtype, device=device).unsqueeze(0)
        f = torch.tensor([0], dtype=torch.long, device=device)

        with torch.no_grad():
            out = model(
                past_values=[x],
                freq=f,
                output_hidden_states=True,
                return_dict=True
            )
            last_hidden = out.hidden_states[-1][0]
            emb = last_hidden.mean(dim=0).float()

        return emb.cpu().numpy()

    except RuntimeError as e:
        if "out of memory" in str(e).lower():
            logger.warning("âš ï¸ GPU memory insufficient, trying CPU fallback...")
            # å¦‚æœGPUå†…å­˜ä¸è¶³ï¼Œå¼ºåˆ¶ä½¿ç”¨CPU
            x = torch.tensor(series, dtype=torch.float32, device="cpu").unsqueeze(0)
            f = torch.tensor([0], dtype=torch.long, device="cpu")

            # å°†æ¨¡å‹ä¸´æ—¶ç§»åˆ°CPU
            model_cpu = model.cpu()
            with torch.no_grad():
                out = model_cpu(
                    past_values=[x],
                    freq=f,
                    output_hidden_states=True,
                    return_dict=True
                )
                last_hidden = out.hidden_states[-1][0]
                emb = last_hidden.mean(dim=0).float()

            return emb.numpy()
        else:
            raise e

def load_and_process_data():
    """åŠ è½½JSONæ•°æ®å¹¶ç”Ÿæˆembeddings - å†…å­˜ä¼˜åŒ–ç‰ˆæœ¬"""
    global records, embeddings, faiss_index

    try:
        # åŠ è½½JSONæ•°æ®
        json_path = os.path.join("dataProcess", "data.json")
        if not os.path.exists(json_path):
            raise FileNotFoundError(f"JSON file not found: {json_path}")

        logger.info("ğŸ“‚ Loading JSON data...")
        with open(json_path, 'r', encoding='utf-8') as f:
            records = json.load(f)

        logger.info(f"âœ… Loaded {len(records)} records from JSON")

        # æ ¹æ®å¯ç”¨å†…å­˜åŠ¨æ€è°ƒæ•´å¤„ç†æ•°é‡
        available_memory_gb = 8  # é»˜è®¤å‡è®¾8GB
        if device_info.get("cuda_available"):
            # GPUæ¨¡å¼å¯ä»¥å¤„ç†æ›´å¤šæ•°æ®
            max_records = min(len(records), 100000)
        else:
            # CPUæ¨¡å¼é™åˆ¶æ•°æ®é‡ä»¥èŠ‚çœå†…å­˜
            max_records = min(len(records), 30000)

        if len(records) > max_records:
            logger.info(f"âš ï¸ Limiting to first {max_records} records for memory efficiency")
            records = records[:max_records]

        # ç”Ÿæˆembeddings - æ‰¹å¤„ç†ä»¥èŠ‚çœå†…å­˜
        logger.info("ğŸ”® Generating embeddings...")
        embeddings_list = []
        batch_size = 100 if device_info.get("type") == "cpu" else 500  # CPUç”¨æ›´å°çš„æ‰¹æ¬¡

        for i in range(0, len(records), batch_size):
            batch_end = min(i + batch_size, len(records))
            logger.info(f"Processing batch {i//batch_size + 1}/{(len(records) + batch_size - 1)//batch_size} (records {i}-{batch_end})")

            batch_embeddings = []
            for j in range(i, batch_end):
                rec = records[j]
                try:
                    # æå–æ•°å€¼ç‰¹å¾
                    series = np.array([
                        float(rec.get("TotalCount", 0)),
                        float(rec.get("LastWeek", 0)),
                        float(rec.get("Previous4DayTimeAvg", 0)),
                        float(rec.get("LastYear", 0)),
                        float(rec.get("Previous52DayTimeAvg", 0))
                    ], dtype=np.float32)

                    # å¤„ç†NaNå€¼
                    series = np.nan_to_num(series, nan=0.0)

                    # ç”Ÿæˆembedding
                    emb = timesfm_embed(series)
                    # æ ‡å‡†åŒ–
                    norm = np.linalg.norm(emb)
                    if norm > 1e-12:
                        emb = emb / norm

                    batch_embeddings.append(emb)

                except Exception as e:
                    logger.warning(f"Error processing record {j}: {str(e)}")
                    # ä½¿ç”¨é›¶å‘é‡ä½œä¸ºfallback
                    emb_dim = 512  # TimesFMçš„embeddingç»´åº¦
                    batch_embeddings.append(np.zeros(emb_dim, dtype=np.float32))

            embeddings_list.extend(batch_embeddings)

            # å®šæœŸæ¸…ç†GPUç¼“å­˜
            if device_info.get("cuda_available"):
                torch.cuda.empty_cache()

        embeddings = np.stack(embeddings_list).astype("float32")
        logger.info(f"âœ… Generated embeddings: {embeddings.shape}")

        # åˆ›å»ºFAISSç´¢å¼•
        logger.info("ğŸ” Creating FAISS index...")
        dim = embeddings.shape[1]
        faiss_index = faiss.IndexFlatIP(dim)  # ä½¿ç”¨å†…ç§¯ç›¸ä¼¼åº¦
        faiss_index.add(embeddings)

        logger.info(f"âœ… FAISS index created with {faiss_index.ntotal} vectors")
        return True

    except Exception as e:
        logger.error(f"âŒ Failed to load and process data: {str(e)}")
        return False

def search_similar_records(query_data: Dict[str, Any], top_k: int = 5) -> List[Dict[str, Any]]:
    """æœç´¢ç›¸ä¼¼çš„è®°å½•"""
    if faiss_index is None or model is None:
        raise ValueError("System not properly initialized")

    try:
        # ä»æŸ¥è¯¢æ•°æ®ä¸­æå–æ•°å€¼ç‰¹å¾
        query_series = np.array([
            float(query_data.get("TotalCount", 0)),
            float(query_data.get("LastWeek", 0)),
            float(query_data.get("Previous4DayTimeAvg", 0)),
            float(query_data.get("LastYear", 0)),
            float(query_data.get("Previous52DayTimeAvg", 0))
        ], dtype=np.float32)

        # å¤„ç†NaNå€¼
        query_series = np.nan_to_num(query_series, nan=0.0)

        # ç”ŸæˆæŸ¥è¯¢embedding
        q_emb = timesfm_embed(query_series)
        norm = np.linalg.norm(q_emb)
        if norm > 1e-12:
            q_emb = q_emb / norm

        q_emb = q_emb.astype("float32").reshape(1, -1)

        # æœç´¢ç›¸ä¼¼è®°å½•
        scores, ids = faiss_index.search(q_emb, top_k)

        # è¿”å›ç›¸ä¼¼è®°å½•
        similar_records = []
        for i, record_id in enumerate(ids[0]):
            if record_id < len(records):
                record = records[record_id].copy()
                record['similarity_score'] = float(scores[0][i])
                similar_records.append(record)

        return similar_records

    except Exception as e:
        logger.error(f"Error in similarity search: {str(e)}")
        return []

def generate_llm_response(query: str, similar_records: List[Dict[str, Any]]) -> str:
    """ä½¿ç”¨LLMç”Ÿæˆå“åº”"""
    global client

    if client is None:
        openai_api_key = os.getenv("OPENAI_API_KEY")
        if not openai_api_key:
            raise ValueError("OPENAI_API_KEY not found in environment variables")
        client = OpenAI(api_key=openai_api_key)

    # æ„å»ºä¸Šä¸‹æ–‡
    context_parts = []
    for i, record in enumerate(similar_records):
        context_parts.append(
            f"{i+1}. {record['Location_Name']} ({record['Date']}): "
            f"TotalCount={record['TotalCount']}, "
            f"LastWeek={record.get('LastWeek', 'N/A')}, "
            f"LastYear={record.get('LastYear', 'N/A')}, "
            f"ç›¸ä¼¼åº¦={record.get('similarity_score', 0):.3f}"
        )

    context = "\n".join(context_parts)

    prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„äººæµé‡åˆ†æåŠ©æ‰‹ã€‚ç”¨æˆ·è¯¢é—®: "{query}"

ä»¥ä¸‹æ˜¯é€šè¿‡å‘é‡ç›¸ä¼¼åº¦æ£€ç´¢åˆ°çš„æœ€ç›¸å…³çš„å†å²è®°å½•ï¼š
{context}

è¯·åŸºäºè¿™äº›æ£€ç´¢åˆ°çš„å†å²æ•°æ®æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚æ³¨æ„ï¼š
1. é‡ç‚¹å…³æ³¨TotalCountï¼ˆæ€»äººæ•°ï¼‰è¿™ä¸ªä¸»è¦æŒ‡æ ‡
2. å¯ä»¥åˆ†ææ—¶é—´è¶‹åŠ¿ã€åœ°ç‚¹å¯¹æ¯”ç­‰
3. å¦‚æœæ•°æ®ä¸­æœ‰LastWeekã€LastYearç­‰å†å²å¯¹æ¯”æ•°æ®ï¼Œå¯ä»¥ç”¨æ¥åˆ†æè¶‹åŠ¿
4. ç”¨ä¸­æ–‡å›ç­”ï¼Œè¯­è¨€è¦ä¸“ä¸šä½†æ˜“æ‡‚
5. å¦‚æœæ£€ç´¢åˆ°çš„æ•°æ®ä¸é—®é¢˜ä¸å¤ªç›¸å…³ï¼Œè¯·è¯šå®è¯´æ˜

è¯·æä¾›æœ‰æ´å¯ŸåŠ›çš„åˆ†æå’Œå›ç­”ã€‚
"""

    try:
        response = client.chat.completions.create(
            model="gpt-4o-mini",
            messages=[{"role": "user", "content": prompt}],
            temperature=0.3
        )
        return response.choices[0].message.content
    except Exception as e:
        logger.error(f"Error generating LLM response: {str(e)}")
        return f"æŠ±æ­‰ï¼Œç”Ÿæˆå›ç­”æ—¶å‡ºç°é”™è¯¯: {str(e)}"

@app.route("/", methods=["GET"])
def health_check():
    """å¥åº·æ£€æŸ¥æ¥å£"""
    status = {
        "service": "RAG-based Footfall Analysis API (CPU Friendly)",
        "status": "running",
        "device_info": device_info,
        "timesfm_loaded": model is not None,
        "faiss_index_loaded": faiss_index is not None,
        "records_count": len(records) if records else 0,
        "embedding_dimension": embeddings.shape[1] if embeddings is not None else None
    }
    return jsonify(status)

@app.route("/ask", methods=["POST"])
def ask_question():
    """RAGæŸ¥è¯¢æ¥å£"""
    try:
        # æ£€æŸ¥ç³»ç»Ÿæ˜¯å¦å·²åˆå§‹åŒ–
        if model is None or faiss_index is None:
            return jsonify({
                "error": "System not initialized. Please check server logs.",
                "success": False
            }), 500

        # è·å–è¯·æ±‚æ•°æ®
        data = request.get_json()
        if not data or "question" not in data:
            return jsonify({
                "error": "Missing 'question' field in request body",
                "success": False
            }), 400

        question = data["question"].strip()
        if not question:
            return jsonify({
                "error": "Question cannot be empty",
                "success": False
            }), 400

        logger.info(f"ğŸ“ Received question: {question}")

        # æå–æŸ¥è¯¢å‚æ•°ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        query_params = data.get("query_params", {})
        top_k = data.get("top_k", 5)

        # å¦‚æœæ²¡æœ‰æä¾›å…·ä½“çš„æŸ¥è¯¢å‚æ•°ï¼Œä½¿ç”¨é»˜è®¤å€¼è¿›è¡Œæœç´¢
        if not query_params:
            # å¯ä»¥ä»é—®é¢˜ä¸­å°è¯•æå–ä¸€äº›ä¿¡æ¯ï¼Œæˆ–ä½¿ç”¨å¹³å‡å€¼
            query_params = {
                "TotalCount": 2000,  # é»˜è®¤å€¼
                "LastWeek": 2000,
                "Previous4DayTimeAvg": 2000,
                "LastYear": 2000,
                "Previous52DayTimeAvg": 2000
            }

        # æœç´¢ç›¸ä¼¼è®°å½•
        similar_records = search_similar_records(query_params, top_k)

        if not similar_records:
            return jsonify({
                "question": question,
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„æ•°æ®è®°å½•ã€‚",
                "success": True,
                "similar_records": []
            })

        # ä½¿ç”¨LLMç”Ÿæˆå›ç­”
        answer = generate_llm_response(question, similar_records)

        logger.info(f"âœ… Generated response for question: {question}")

        return jsonify({
            "question": question,
            "answer": answer,
            "success": True,
            "similar_records": similar_records[:3],  # è¿”å›å‰3ä¸ªæœ€ç›¸ä¼¼çš„è®°å½•
            "total_records": len(records),
            "device_used": device_info.get("type", "unknown")
        })

    except Exception as e:
        logger.error(f"âŒ Error processing question: {str(e)}")
        return jsonify({
            "error": str(e),
            "success": False
        }), 500

@app.route("/search", methods=["POST"])
def search_similar():
    """åŸºäºæ•°å€¼ç‰¹å¾çš„ç›¸ä¼¼åº¦æœç´¢æ¥å£"""
    try:
        data = request.get_json()
        if not data:
            return jsonify({
                "error": "Request body is required",
                "success": False
            }), 400

        top_k = data.get("top_k", 5)
        query_params = data.get("query_params", {})

        if not query_params:
            return jsonify({
                "error": "query_params is required",
                "success": False
            }), 400

        similar_records = search_similar_records(query_params, top_k)

        return jsonify({
            "similar_records": similar_records,
            "success": True,
            "query_params": query_params,
            "device_used": device_info.get("type", "unknown")
        })

    except Exception as e:
        logger.error(f"âŒ Error in similarity search: {str(e)}")
        return jsonify({
            "error": str(e),
            "success": False
        }), 500

@app.route("/data/info", methods=["GET"])
def data_info():
    """è·å–æ•°æ®é›†ä¿¡æ¯çš„æ¥å£"""
    if not records:
        return jsonify({
            "error": "Data not loaded",
            "success": False
        }), 500

    # è®¡ç®—ä¸€äº›ç»Ÿè®¡ä¿¡æ¯
    sample_records = records[:3] if len(records) >= 3 else records

    return jsonify({
        "total_records": len(records),
        "embedding_dimension": embeddings.shape[1] if embeddings is not None else None,
        "faiss_index_size": faiss_index.ntotal if faiss_index is not None else None,
        "sample_records": sample_records,
        "available_fields": list(records[0].keys()) if records else [],
        "device_info": device_info,
        "success": True
    })

@app.errorhandler(404)
def not_found(error):
    """404é”™è¯¯å¤„ç†"""
    return jsonify({
        "error": "Endpoint not found",
        "success": False,
        "available_endpoints": ["/", "/ask", "/search", "/data/info"]
    }), 404

@app.errorhandler(500)
def internal_error(error):
    """500é”™è¯¯å¤„ç†"""
    return jsonify({
        "error": "Internal server error",
        "success": False
    }), 500

def initialize_system():
    """åˆå§‹åŒ–æ•´ä¸ªç³»ç»Ÿ"""
    logger.info("ğŸš€ Initializing RAG-based Footfall Analysis System (CPU Friendly)...")

    # æ£€æµ‹è®¾å¤‡
    detect_device()

    # åˆå§‹åŒ–TimesFMæ¨¡å‹
    if not initialize_timesfm():
        return False

    # åŠ è½½æ•°æ®å¹¶ç”Ÿæˆembeddings
    if not load_and_process_data():
        return False

    logger.info("âœ… System initialization completed successfully!")
    logger.info(f"ğŸ“Š Final stats: {len(records)} records, device: {device_info.get('type', 'unknown')}")
    return True

if __name__ == "__main__":
    try:
        # åˆå§‹åŒ–ç³»ç»Ÿ
        if not initialize_system():
            raise Exception("System initialization failed")

        # å¯åŠ¨FlaskæœåŠ¡
        logger.info("ğŸš€ Starting RAG-based Footfall Analysis API Server (CPU Friendly)...")
        app.run(
            host="0.0.0.0",
            port=5080,
            debug=False,
            threaded=True
        )

    except Exception as e:
        logger.error(f"âŒ Failed to start server: {str(e)}")
        print(f"\nâŒ Server startup failed: {str(e)}")
        print("Please check:")
        print("1. OPENAI_API_KEY is set in .env file")
        print("2. dataProcess/data.json file exists")
        print("3. All required packages are installed (pip install -r requirements.txt)")
        print("4. Sufficient system memory (8GB+ recommended)")
        print("5. Internet connection for downloading TimesFM model")
